{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7dd300-5e88-4074-8bda-e8eccf585e79",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762954536153}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>2</td></tr><tr><td>null</td></tr><tr><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ],
        [
         null
        ],
        [
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1=[(1,),(1,),(2,),(None,),(None,)]\n",
    "data2=[(1,),(3,),(None,)]\n",
    "\n",
    "df1=spark.createDataFrame(data1,schema=['id',])\n",
    "df2=spark.createDataFrame(data2,schema=['id',])\n",
    "df3=df1.join(df2,df1['id']==df2['id'],'anti')\n",
    "df3.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39355f0-fdb3-4480-9e6f-4b047b3a62c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Find the total marks of the top 2 subjects for each student based on their marks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d82350-4aaf-4749-85dd-4a3d565e0391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sname</th><th>sid</th><th>marks</th></tr></thead><tbody><tr><td>A</td><td>X</td><td>75</td></tr><tr><td>A</td><td>Y</td><td>75</td></tr><tr><td>A</td><td>Z</td><td>80</td></tr><tr><td>B</td><td>X</td><td>90</td></tr><tr><td>B</td><td>X</td><td>91</td></tr><tr><td>B</td><td>X</td><td>75</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         "X",
         75
        ],
        [
         "A",
         "Y",
         75
        ],
        [
         "A",
         "Z",
         80
        ],
        [
         "B",
         "X",
         90
        ],
        [
         "B",
         "X",
         91
        ],
        [
         "B",
         "X",
         75
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "marks",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sname</th><th>sid</th><th>marks</th><th>rn</th></tr></thead><tbody><tr><td>A</td><td>Z</td><td>80</td><td>1</td></tr><tr><td>A</td><td>X</td><td>75</td><td>2</td></tr><tr><td>A</td><td>Y</td><td>75</td><td>3</td></tr><tr><td>B</td><td>X</td><td>91</td><td>1</td></tr><tr><td>B</td><td>X</td><td>90</td><td>2</td></tr><tr><td>B</td><td>X</td><td>75</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         "Z",
         80,
         1
        ],
        [
         "A",
         "X",
         75,
         2
        ],
        [
         "A",
         "Y",
         75,
         3
        ],
        [
         "B",
         "X",
         91,
         1
        ],
        [
         "B",
         "X",
         90,
         2
        ],
        [
         "B",
         "X",
         75,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "marks",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "rn",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sname</th><th>sum(marks)</th></tr></thead><tbody><tr><td>A</td><td>155</td></tr><tr><td>B</td><td>181</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         155
        ],
        [
         "B",
         181
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sname",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "sum(marks)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[('A','X',75),('A','Y',75),('A','Z',80),('B','X',90),('B','X',91),('B','X',75),]\n",
    "\n",
    "cols=['sname','sid','marks']\n",
    "df=spark.createDataFrame(data,cols)\n",
    "df.display()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,sum\n",
    "df1=df.withColumn('rn',row_number().over(Window.partitionBy('sname').orderBy(df['marks'].desc())))\n",
    "df2=df1.filter(df1['rn']<=2).groupBy('sname').agg(sum('marks'))\n",
    "df1.display()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f650334-4930-42fc-91e8-a8db1e69777d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given an employees table with duplicate IDs. Find the maximum ID while excluding duplicates. (if Max id having duplicate then that should not be considered) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634c27fb-04df-4f9f-8573-dd28842ae57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  8|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(2,),(5,),(6,),(6,),(7,),(8,),(8,)]\n",
    "df=spark.createDataFrame(data,schema=['id',])\n",
    "#df.display()\n",
    "from pyspark.sql.functions import col\n",
    "dist_df=df.distinct().sort(col('id').desc()).limit(1)\n",
    "dist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1453463-0b1f-4e4a-ba94-f67e3ab1fcdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.Scenario: You need to combine two tables: the Person table and the Address table. The goal is to display the first name, last name, city, and state for each person. If a person's address is not found, you should return NULL for the city and state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0824c612-a5b3-49ac-b67c-59a70d80272b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>personId</th><th>firstName</th><th>lastName</th><th>addressId</th><th>personId</th><th>city</th><th>state</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>Doe</td><td>101</td><td>1</td><td>New York</td><td>NY</td></tr><tr><td>2</td><td>Jane</td><td>Smith</td><td>102</td><td>2</td><td>Chicago</td><td>IL</td></tr><tr><td>3</td><td>Alice</td><td>Brown</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "Doe",
         101,
         1,
         "New York",
         "NY"
        ],
        [
         2,
         "Jane",
         "Smith",
         102,
         2,
         "Chicago",
         "IL"
        ],
        [
         3,
         "Alice",
         "Brown",
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "personId",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "addressId",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "personId",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "person_data = [\n",
    "    (1, \"John\", \"Doe\"),\n",
    "    (2, \"Jane\", \"Smith\"),\n",
    "    (3, \"Alice\", \"Brown\")\n",
    "]\n",
    "person_columns = [\"personId\", \"firstName\", \"lastName\"]\n",
    "\n",
    "person_df = spark.createDataFrame(person_data, schema=person_columns)\n",
    "\n",
    "# -------- Address DataFrame --------\n",
    "address_data = [\n",
    "    (101, 1, \"New York\", \"NY\"),\n",
    "    (102, 2, \"Chicago\", \"IL\")\n",
    "]\n",
    "address_columns = [\"addressId\", \"personId\", \"city\", \"state\"]\n",
    "\n",
    "address_df =spark.createDataFrame(address_data, schema=address_columns)\n",
    "\n",
    "resultent_df=person_df.join(address_df,person_df['personId']==address_df['personId'],'left')\n",
    "resultent_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124c2895-1ea9-4f37-9985-841ef2783c04",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763023336394}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>num</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>1</td></tr><tr><td>3</td><td>1</td></tr><tr><td>4</td><td>2</td></tr><tr><td>5</td><td>1</td></tr><tr><td>6</td><td>2</td></tr><tr><td>7</td><td>2</td></tr><tr><td>8</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1
        ],
        [
         2,
         1
        ],
        [
         3,
         1
        ],
        [
         4,
         2
        ],
        [
         5,
         1
        ],
        [
         6,
         2
        ],
        [
         7,
         2
        ],
        [
         8,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>num</th><th>lag_col</th><th>lead_col</th></tr></thead><tbody><tr><td>2</td><td>1</td><td>1</td><td>1</td></tr><tr><td>7</td><td>2</td><td>2</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         1,
         1,
         1
        ],
        [
         7,
         2,
         2,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "lag_col",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "lead_col",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1,1),(2,1),(3,1),(4,2),(5,1),(6,2),(7,2),(8,2)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"num\"])\n",
    "df.display()\n",
    "from pyspark.sql.functions import lag,lead\n",
    "from pyspark.sql import Window\n",
    "windowsp_lg=lag('num').over(Window.orderBy('id'))\n",
    "windowsp_ld=lead('num').over(Window.orderBy('id'))\n",
    "df1=df.withColumn('lag_col',windowsp_lg)\n",
    "df2=df1.withColumn('lead_col',windowsp_ld)\n",
    "df3=df2.filter((df2['num']==df2['lag_col'] )& (df2['lag_col']==df2['lead_col'])).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bacaf1b-32ed-4925-80e9-cd459260c320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Customers DataFrame ---\n+-----------+----------+---------+--------------------+\n|customer_id|first_name|last_name|               email|\n+-----------+----------+---------+--------------------+\n|          1|      John|      Doe|john.doe@example.com|\n|          2|      Jane|    Smith|jane.smith@exampl...|\n|          3|       Bob|  Johnson|bob.johnson@examp...|\n|          4|     Alice|    White|alice.white@examp...|\n+-----------+----------+---------+--------------------+\n\nroot\n |-- customer_id: integer (nullable = false)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n\n\n--- Orders DataFrame ---\n+--------+-----------+-------------------+\n|order_id|customer_id|         order_date|\n+--------+-----------+-------------------+\n|     101|          1|2022-07-01 10:00:00|\n|     102|          3|2022-07-05 12:30:00|\n+--------+-----------+-------------------+\n\nroot\n |-- order_id: integer (nullable = false)\n |-- customer_id: integer (nullable = false)\n |-- order_date: timestamp (nullable = false)\n\n\n--- Joined DataFrame (Customer & Order Info) ---\n+-----------+----------+---------+--------------------+\n|customer_id|first_name|last_name|               email|\n+-----------+----------+---------+--------------------+\n|          2|      Jane|    Smith|jane.smith@exampl...|\n|          4|     Alice|    White|alice.white@examp...|\n+-----------+----------+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Customers DataFrame ---\n",
    "customers_data = [\n",
    "    (1, \"John\", \"Doe\", \"john.doe@example.com\"),\n",
    "    (2, \"Jane\", \"Smith\", \"jane.smith@example.com\"),\n",
    "    (3, \"Bob\", \"Johnson\", \"bob.johnson@example.com\"),\n",
    "    (4, \"Alice\", \"White\", \"alice.white@example.com\")\n",
    "]\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(data=customers_data, schema=customers_schema)\n",
    "\n",
    "print(\"--- Customers DataFrame ---\")\n",
    "customers_df.show()\n",
    "customers_df.printSchema()\n",
    "\n",
    "# --- 2. Orders DataFrame ---\n",
    "# Convert date strings to datetime objects for TimestampType\n",
    "orders_data = [\n",
    "    (101, 1, datetime.strptime(\"2022-07-01 10:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (102, 3, datetime.strptime(\"2022-07-05 12:30:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(data=orders_data, schema=orders_schema)\n",
    "\n",
    "print(\"\\n--- Orders DataFrame ---\")\n",
    "orders_df.show()\n",
    "orders_df.printSchema()\n",
    "\n",
    "# Example of joining the two DataFrames\n",
    "print(\"\\n--- Joined DataFrame (Customer & Order Info) ---\")\n",
    "joined_df = customers_df.join(orders_df, \"customer_id\", \"anti\")\n",
    "joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c29e9d8f-c607-4490-9985-5def7dba42e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "interchange id with next one like 1 to 2 and 2 to 1 but if last one is odd then dont change as we dont have next value for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd50d57-ebdd-480b-bdea-c12760b83eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:560: UserWarning: 'verifySchema' is ignored. It is not supported with Spark Connect.\n  warnings.warn(\"'verifySchema' is ignored. It is not supported with Spark Connect.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr><tr><td>3</td><td>Charlie</td></tr><tr><td>4</td><td>David</td></tr><tr><td>5</td><td>Eve</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ],
        [
         3,
         "Charlie"
        ],
        [
         4,
         "David"
        ],
        [
         5,
         "Eve"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|   name|\n+---+-------+\n|  2|  Alice|\n|  1|    Bob|\n|  4|Charlie|\n|  3|  David|\n|  5|    Eve|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import when,col\n",
    "spark=SparkSession.builder.appName('spark_practice').getOrCreate()\n",
    "spark\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\"),\n",
    "    (5, \"Eve\")\n",
    "]\n",
    "\n",
    "# 3. Define columns\n",
    "columns = StructType([StructField('id',IntegerType()),StructField('name',StringType())])\n",
    "spark_df=spark.createDataFrame(data,columns,verifySchema=True)\n",
    "spark_df.display()\n",
    "row=spark_df.orderBy(col('id').desc()).limit(1).collect()[0]['id']\n",
    "updated_df=spark_df.withColumn('id',when((col('id')%2==1) &(col('id')!=row),col('id')+1).when((col('id')%2==0),col('id')-1).otherwise(col('id')))\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2860d3-58a2-4a8f-bbc6-cb2f598bb006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>f</td></tr><tr><td>2</td><td>m</td></tr><tr><td>3</td><td>f</td></tr><tr><td>4</td><td>m</td></tr><tr><td>5</td><td>m</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "f"
        ],
        [
         2,
         "m"
        ],
        [
         3,
         "f"
        ],
        [
         4,
         "m"
        ],
        [
         5,
         "m"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>m</td></tr><tr><td>2</td><td>f</td></tr><tr><td>3</td><td>m</td></tr><tr><td>4</td><td>f</td></tr><tr><td>5</td><td>f</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "m"
        ],
        [
         2,
         "f"
        ],
        [
         3,
         "m"
        ],
        [
         4,
         "f"
        ],
        [
         5,
         "f"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "data = [\n",
    "    (1, \"f\"),\n",
    "    (2, \"m\"),\n",
    "    (3, \"f\"),\n",
    "    (4, \"m\"),\n",
    "    (5, \"m\")\n",
    "]\n",
    "\n",
    "schema=StructType([StructField('id',IntegerType(),False),StructField('name',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.display()\n",
    "\n",
    "df.withColumn('name',when(col('name')=='f','m').otherwise('f')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f03763-fb53-4d57-a69c-fa2573c9759c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763136167589}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>a</th><th>b</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>201</td></tr><tr><td>2</td><td>102</td><td>202</td></tr><tr><td>3</td><td>101</td><td>201</td></tr><tr><td>4</td><td>103</td><td>203</td></tr><tr><td>5</td><td>101</td><td>201</td></tr><tr><td>6</td><td>101</td><td>202</td></tr><tr><td>7</td><td>102</td><td>202</td></tr><tr><td>8</td><td>101</td><td>202</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "101",
         "201"
        ],
        [
         2,
         "102",
         "202"
        ],
        [
         3,
         "101",
         "201"
        ],
        [
         4,
         "103",
         "203"
        ],
        [
         5,
         "101",
         "201"
        ],
        [
         6,
         "101",
         "202"
        ],
        [
         7,
         "102",
         "202"
        ],
        [
         8,
         "101",
         "202"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th><th>rn</th></tr></thead><tbody><tr><td>101</td><td>201</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "201",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rn",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies_data = [\n",
    "    (1, 101, 201),\n",
    "    (2, 102, 202),\n",
    "    (3, 101, 201),\n",
    "    (4, 103, 203),\n",
    "    (5, 101, 201),\n",
    "    (6, 101, 202),\n",
    "    (7, 102, 202),\n",
    "    (8, 101, 202)\n",
    "]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "schema=StructType([StructField('id',IntegerType()),StructField('a',StringType()),StructField('b',StringType())])\n",
    "\n",
    "movie_df=spark.createDataFrame(movies_data,schema)\n",
    "movie_df.display()\n",
    "upd_df=movie_df.groupBy('a','b').agg(count('*').alias('rn'))\n",
    "upd_df.filter(col('rn')>2).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c325b651-e600-48b4-9803-42e405b3b603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "next question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc807c1a-0899-4395-9694-9652ae2d923b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+direct_to_type_checking": {}
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6987427263989570>, line 44\u001B[0m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TaskContext\n",
       "\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mPrintJobStageTaskListener\u001B[39;00m(SparkContext\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mSparkListener):\n",
       "\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21monJobStart\u001B[39m(\u001B[38;5;28mself\u001B[39m, jobStart):\n",
       "\u001B[1;32m     46\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== JOB STARTED: Job ID = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjobStart\u001B[38;5;241m.\u001B[39mjobId()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'org'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'NoneType' object has no attribute 'org'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'org'"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KD00G",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-6987427263989570>, line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TaskContext\n\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mPrintJobStageTaskListener\u001B[39;00m(SparkContext\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mSparkListener):\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21monJobStart\u001B[39m(\u001B[38;5;28mself\u001B[39m, jobStart):\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m=== JOB STARTED: Job ID = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjobStart\u001B[38;5;241m.\u001B[39mjobId()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'org'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import TaskContext\n",
    "\n",
    "from pyspark import Accumulator\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import AccumulatorParam\n",
    "\n",
    "from pyspark import TaskContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import TaskContext\n",
    "\n",
    "from pyspark import TaskContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# ----------------------------\n",
    "# Custom Listener\n",
    "# ----------------------------\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import TaskContext\n",
    "\n",
    "class PrintJobStageTaskListener(SparkContext._jvm.org.apache.spark.scheduler.SparkListener):\n",
    "    def onJobStart(self, jobStart):\n",
    "        print(f\"\\n=== JOB STARTED: Job ID = {jobStart.jobId()} ===\")\n",
    "\n",
    "    def onStageSubmitted(self, stageSubmitted):\n",
    "        stage_info = stageSubmitted.stageInfo()\n",
    "        print(f\"  -> Stage Submitted: Stage ID = {stage_info.stageId()}, Name = {stage_info.name()}\")\n",
    "\n",
    "    def onTaskStart(self, taskStart):\n",
    "        info = taskStart.taskInfo()\n",
    "        print(f\"      -> Task Start: Task ID = {info.taskId()}, Partition = {info.index()}\")\n",
    "\n",
    "    def onTaskEnd(self, taskEnd):\n",
    "        info = taskEnd.taskInfo()\n",
    "        print(f\"      -> Task End: Task ID = {info.taskId()}, Success = {info.successful()}\")\n",
    "\n",
    "    def onStageCompleted(self, stageCompleted):\n",
    "        info = stageCompleted.stageInfo()\n",
    "        print(f\"  -> Stage Completed: Stage ID = {info.stageId()}\")\n",
    "\n",
    "    def onJobEnd(self, jobEnd):\n",
    "        print(f\"=== JOB ENDED: Job ID = {jobEnd.jobId()} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73e98de-efe5-4d9b-9093-4e0e03911946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+direct_to_type_checking": {}
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6987427263989569>, line 13\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m dataupd \u001B[38;5;241m=\u001B[39m data_df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39magg(\n",
       "\u001B[1;32m      6\u001B[0m         \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_date\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmini_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_date\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxi_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m     ) \\\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate_diff\u001B[39m\u001B[38;5;124m\"\u001B[39m, datediff(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxi_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmini_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\u001B[1;32m     11\u001B[0m dataupd\u001B[38;5;241m.\u001B[39mcount()   \u001B[38;5;66;03m# triggers a job\u001B[39;00m\n",
       "\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataupd\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mqueryExecution()\u001B[38;5;241m.\u001B[39mlogical())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1706\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   1704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1705\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jdf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jmap\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jcols\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoJSON\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m-> 1706\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m   1707\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m   1708\u001B[0m         )\n",
       "\u001B[1;32m   1710\u001B[0m     \u001B[38;5;66;03m# BEGIN-EDGE\u001B[39;00m\n",
       "\u001B[1;32m   1711\u001B[0m     \u001B[38;5;66;03m# SC-148106: skip .columns call in __getattr__ for IPython rich display methods.\u001B[39;00m\n",
       "\u001B[1;32m   1712\u001B[0m     \u001B[38;5;66;03m# We assume that no users define columns with these names and expect to access them\u001B[39;00m\n",
       "\u001B[1;32m   1713\u001B[0m     \u001B[38;5;66;03m# via __getattr__:\u001B[39;00m\n",
       "\u001B[1;32m   1714\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\n",
       "\u001B[1;32m   1715\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ipython_canary_method_should_not_exist_\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1716\u001B[0m         \u001B[38;5;66;03m# See: https://ipython.readthedocs.io/en/stable/config/integrating.html#rich-display\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1726\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_repr_svg_\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1727\u001B[0m     ]:\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "metadata": {
        "errorSummary": "[JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "JVM_ATTRIBUTE_NOT_SUPPORTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-6987427263989569>, line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m dataupd \u001B[38;5;241m=\u001B[39m data_df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39magg(\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_date\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmini_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_date\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxi_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m     ) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate_diff\u001B[39m\u001B[38;5;124m\"\u001B[39m, datediff(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxi_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmini_dt\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m     11\u001B[0m dataupd\u001B[38;5;241m.\u001B[39mcount()   \u001B[38;5;66;03m# triggers a job\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataupd\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mqueryExecution()\u001B[38;5;241m.\u001B[39mlogical())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1706\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1705\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jdf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jmap\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jcols\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoJSON\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m-> 1706\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m   1707\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJVM_ATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m   1708\u001B[0m         )\n\u001B[1;32m   1710\u001B[0m     \u001B[38;5;66;03m# BEGIN-EDGE\u001B[39;00m\n\u001B[1;32m   1711\u001B[0m     \u001B[38;5;66;03m# SC-148106: skip .columns call in __getattr__ for IPython rich display methods.\u001B[39;00m\n\u001B[1;32m   1712\u001B[0m     \u001B[38;5;66;03m# We assume that no users define columns with these names and expect to access them\u001B[39;00m\n\u001B[1;32m   1713\u001B[0m     \u001B[38;5;66;03m# via __getattr__:\u001B[39;00m\n\u001B[1;32m   1714\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[1;32m   1715\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ipython_canary_method_should_not_exist_\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1716\u001B[0m         \u001B[38;5;66;03m# See: https://ipython.readthedocs.io/en/stable/config/integrating.html#rich-display\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1726\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_repr_svg_\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1727\u001B[0m     ]:\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [JVM_ATTRIBUTE_NOT_SUPPORTED] Directly accessing the underlying Spark driver JVM using the attribute '_jdf' is not supported on serverless compute. If you require direct access to these fields, consider using a single-user cluster. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max, datediff\n",
    "\n",
    "# run your action\n",
    "dataupd = data_df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        min(\"post_date\").alias(\"mini_dt\"),\n",
    "        max(\"post_date\").alias(\"maxi_dt\")\n",
    "    ) \\\n",
    "    .withColumn(\"date_diff\", datediff(col(\"maxi_dt\"), col(\"mini_dt\")))\n",
    "\n",
    "dataupd.count()   # triggers a job\n",
    "\n",
    "print(dataupd._jdf.queryExecution().logical())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-12 18:38:40",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}